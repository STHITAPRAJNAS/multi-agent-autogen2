{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Agentic System\n",
    "\n",
    "This notebook tests the multi-agent system by:\n",
    "1. Setting up the environment\n",
    "2. Hydrating the knowledge base with example documents\n",
    "3. Testing each agent's functionality\n",
    "4. Testing the orchestrator with complex queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add src to Python path\n",
    "src_path = str(Path.cwd() / 'src')\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n",
    "\n",
    "from autogen_app.agents import create_agents\n",
    "from autogen_app.vector_store import get_vector_store, load_config\n",
    "from langchain.schema import Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up environment variables from .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment: local\n",
      "AWS Access Key ID: Set\n",
      "AWS Secret Access Key: Set\n",
      "Gemini API Key: Set\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Verify environment variables are loaded\n",
    "print(f\"Environment: {os.getenv('ENV')}\")\n",
    "print(f\"AWS Access Key ID: {'Set' if os.getenv('AWS_ACCESS_KEY_ID') else 'Not Set'}\")\n",
    "print(f\"AWS Secret Access Key: {'Set' if os.getenv('AWS_SECRET_ACCESS_KEY') else 'Not Set'}\")\n",
    "print(f\"Gemini API Key: {'Set' if os.getenv('GEMINI_API_KEY') else 'Not Set'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hydrate the knowledge base with example documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sthitaprajnasahoo/Developer/local_learning/multi-agent-autogen2/src/autogen_app/vector_store.py:31: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  self.embeddings = HuggingFaceEmbeddings(\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1\" 200 6739\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6739\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1\" 200 6739\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6739\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: mps\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config_sentence_transformers.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/README.md HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/modules.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/sentence_bert_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/adapter_config.json HTTP/1.1\" 404 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /sentence-transformers/all-MiniLM-L6-v2/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2/revision/main HTTP/1.1\" 200 6739\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/models/sentence-transformers/all-MiniLM-L6-v2 HTTP/1.1\" 200 6739\n",
      "DEBUG:faiss.loader:Environment variable FAISS_OPT_LEVEL is not set, so let's pick the instruction set according to the current CPU\n",
      "INFO:faiss.loader:Loading faiss.\n",
      "INFO:faiss.loader:Successfully loaded faiss.\n",
      "INFO:faiss:Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base hydrated successfully!\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config = load_config()\n",
    "\n",
    "# Get vector stores\n",
    "knowledge_store = get_vector_store(config['vector_stores']['knowledge_base'])\n",
    "sql_store = get_vector_store(config['vector_stores']['databricks_schema'])\n",
    "graphql_store = get_vector_store(config['vector_stores']['graphql_schema'])\n",
    "\n",
    "# Example Confluence knowledge documents\n",
    "confluence_docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Project Overview\n",
    "        Our project uses a microservices architecture with the following components:\n",
    "        - User Service: Handles user authentication and profile management\n",
    "        - Order Service: Manages order processing and fulfillment\n",
    "        - Payment Service: Handles payment processing and transactions\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"confluence\", \"title\": \"Project Architecture\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        API Documentation\n",
    "        The User Service exposes the following endpoints:\n",
    "        - POST /api/users: Create new user\n",
    "        - GET /api/users/{id}: Get user details\n",
    "        - PUT /api/users/{id}: Update user information\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"confluence\", \"title\": \"User Service API\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Example Databricks schema documents\n",
    "databricks_docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Users Table Schema\n",
    "        CREATE TABLE users (\n",
    "            user_id INT PRIMARY KEY,\n",
    "            username VARCHAR(50),\n",
    "            email VARCHAR(100),\n",
    "            created_at TIMESTAMP\n",
    "        )\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"databricks\", \"table\": \"users\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Orders Table Schema\n",
    "        CREATE TABLE orders (\n",
    "            order_id INT PRIMARY KEY,\n",
    "            user_id INT,\n",
    "            total_amount DECIMAL(10,2),\n",
    "            status VARCHAR(20),\n",
    "            created_at TIMESTAMP,\n",
    "            FOREIGN KEY (user_id) REFERENCES users(user_id)\n",
    "        )\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"databricks\", \"table\": \"orders\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Example GraphQL schema documents\n",
    "graphql_docs = [\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        User Type\n",
    "        type User {\n",
    "            id: ID!\n",
    "            username: String!\n",
    "            email: String!\n",
    "            orders: [Order!]!\n",
    "        }\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"graphql\", \"type\": \"User\"}\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"\"\"\n",
    "        Order Type\n",
    "        type Order {\n",
    "            id: ID!\n",
    "            user: User!\n",
    "            totalAmount: Float!\n",
    "            status: String!\n",
    "            createdAt: DateTime!\n",
    "        }\n",
    "        \"\"\",\n",
    "        metadata={\"source\": \"graphql\", \"type\": \"Order\"}\n",
    "    )\n",
    "]\n",
    "\n",
    "# Add documents to vector stores\n",
    "knowledge_store.add_documents(confluence_docs)\n",
    "sql_store.add_documents(databricks_docs)\n",
    "graphql_store.add_documents(graphql_docs)\n",
    "\n",
    "print(\"Knowledge base hydrated successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge store has 2 documents with embeddings\n"
     ]
    }
   ],
   "source": [
    "# Check if knowledge store has embeddings\n",
    "if hasattr(knowledge_store, 'vectorstore') and knowledge_store.vectorstore is not None:\n",
    "    # Check if the vectorstore has any documents\n",
    "    if hasattr(knowledge_store.vectorstore, 'index_to_docstore_id'):\n",
    "        num_docs = len(knowledge_store.vectorstore.index_to_docstore_id)\n",
    "        print(f\"Knowledge store has {num_docs} documents with embeddings\")\n",
    "    else:\n",
    "        print(\"Knowledge store vectorstore exists but has no documents\")\n",
    "else:\n",
    "    print(\"Knowledge store has no vectorstore initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Initializing KnowledgeRetrieverAgent with config: {'database_config': {'db_type': 'sqlite', 'db_url': 'sqlite:///local.db', 'pg_db_url': 'postgresql://user:password@localhost:5432/mydatabase', 'pg_collection_name': 'default_collection'}, 'llm_config': {'max_tokens': 2048, 'temperature': 0.7, 'OAI_CONFIG_LIST': '', 'DEFAULT_MODEL': 'gemini-pro'}, 'conversation_config': {'history_length': 10, 'cost_per_token': 2e-06}, 'logging_config': {'level': 'DEBUG', 'log_file': 'app.log'}, 'bedrock_config': {'ANTHROPIC_MODEL_ID': 'anthropic.claude-v2', 'GEMINI_MODEL_ID': 'google_gemini_pro'}, 'llm': {'provider': 'gemini', 'model': 'gemini-pro', 'api_key': '${GEMINI_API_KEY}', 'api_type': 'google'}, 'vector_stores': {'knowledge_base': {'type': 'memory', 'collection_name': 'knowledge_base', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'databricks_schema': {'type': 'memory', 'collection_name': 'databricks_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'graphql_schema': {'type': 'memory', 'collection_name': 'graphql_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}}, 'agents': {'knowledge_retriever': {'system_prompt': 'You are a knowledge retrieval expert. Use the provided context to answer questions accurately.', 'max_tokens': 4000, 'temperature': 0.1}, 'sql_generator': {'system_prompt': 'You are a SQL expert. Generate accurate SQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'graphql_generator': {'system_prompt': 'You are a GraphQL expert. Generate accurate GraphQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'code_generator': {'system_prompt': 'You are an expert software architect and developer. Generate high-quality, production-ready code.', 'max_tokens': 4000, 'temperature': 0.1}, 'orchestrator': {'system_prompt': 'You are an AI orchestrator. Analyze user questions and delegate tasks to appropriate agents.', 'max_tokens': 4000, 'temperature': 0.1}}}\n",
      "DEBUG:autogen_app.agents:Vector store initialized: <autogen_app.vector_store.MemoryVectorStore object at 0x143117500>\n",
      "DEBUG:autogen_app.agents:Initializing SQLGeneratorAgent with config: {'database_config': {'db_type': 'sqlite', 'db_url': 'sqlite:///local.db', 'pg_db_url': 'postgresql://user:password@localhost:5432/mydatabase', 'pg_collection_name': 'default_collection'}, 'llm_config': {'max_tokens': 2048, 'temperature': 0.7, 'OAI_CONFIG_LIST': '', 'DEFAULT_MODEL': 'gemini-pro'}, 'conversation_config': {'history_length': 10, 'cost_per_token': 2e-06}, 'logging_config': {'level': 'DEBUG', 'log_file': 'app.log'}, 'bedrock_config': {'ANTHROPIC_MODEL_ID': 'anthropic.claude-v2', 'GEMINI_MODEL_ID': 'google_gemini_pro'}, 'llm': {'provider': 'gemini', 'model': 'gemini-pro', 'api_key': '${GEMINI_API_KEY}', 'api_type': 'google'}, 'vector_stores': {'knowledge_base': {'type': 'memory', 'collection_name': 'knowledge_base', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'databricks_schema': {'type': 'memory', 'collection_name': 'databricks_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'graphql_schema': {'type': 'memory', 'collection_name': 'graphql_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}}, 'agents': {'knowledge_retriever': {'system_prompt': 'You are a knowledge retrieval expert. Use the provided context to answer questions accurately.', 'max_tokens': 4000, 'temperature': 0.1}, 'sql_generator': {'system_prompt': 'You are a SQL expert. Generate accurate SQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'graphql_generator': {'system_prompt': 'You are a GraphQL expert. Generate accurate GraphQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'code_generator': {'system_prompt': 'You are an expert software architect and developer. Generate high-quality, production-ready code.', 'max_tokens': 4000, 'temperature': 0.1}, 'orchestrator': {'system_prompt': 'You are an AI orchestrator. Analyze user questions and delegate tasks to appropriate agents.', 'max_tokens': 4000, 'temperature': 0.1}}}\n",
      "DEBUG:autogen_app.agents:Vector store initialized: <autogen_app.vector_store.MemoryVectorStore object at 0x168928e30>\n",
      "DEBUG:autogen_app.agents:Initializing GraphQLGeneratorAgent with config: {'database_config': {'db_type': 'sqlite', 'db_url': 'sqlite:///local.db', 'pg_db_url': 'postgresql://user:password@localhost:5432/mydatabase', 'pg_collection_name': 'default_collection'}, 'llm_config': {'max_tokens': 2048, 'temperature': 0.7, 'OAI_CONFIG_LIST': '', 'DEFAULT_MODEL': 'gemini-pro'}, 'conversation_config': {'history_length': 10, 'cost_per_token': 2e-06}, 'logging_config': {'level': 'DEBUG', 'log_file': 'app.log'}, 'bedrock_config': {'ANTHROPIC_MODEL_ID': 'anthropic.claude-v2', 'GEMINI_MODEL_ID': 'google_gemini_pro'}, 'llm': {'provider': 'gemini', 'model': 'gemini-pro', 'api_key': '${GEMINI_API_KEY}', 'api_type': 'google'}, 'vector_stores': {'knowledge_base': {'type': 'memory', 'collection_name': 'knowledge_base', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'databricks_schema': {'type': 'memory', 'collection_name': 'databricks_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'graphql_schema': {'type': 'memory', 'collection_name': 'graphql_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}}, 'agents': {'knowledge_retriever': {'system_prompt': 'You are a knowledge retrieval expert. Use the provided context to answer questions accurately.', 'max_tokens': 4000, 'temperature': 0.1}, 'sql_generator': {'system_prompt': 'You are a SQL expert. Generate accurate SQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'graphql_generator': {'system_prompt': 'You are a GraphQL expert. Generate accurate GraphQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'code_generator': {'system_prompt': 'You are an expert software architect and developer. Generate high-quality, production-ready code.', 'max_tokens': 4000, 'temperature': 0.1}, 'orchestrator': {'system_prompt': 'You are an AI orchestrator. Analyze user questions and delegate tasks to appropriate agents.', 'max_tokens': 4000, 'temperature': 0.1}}}\n",
      "DEBUG:autogen_app.agents:Vector store initialized: <autogen_app.vector_store.MemoryVectorStore object at 0x172482c00>\n",
      "DEBUG:autogen_app.agents:Initializing OrchestratorAgent with config: {'database_config': {'db_type': 'sqlite', 'db_url': 'sqlite:///local.db', 'pg_db_url': 'postgresql://user:password@localhost:5432/mydatabase', 'pg_collection_name': 'default_collection'}, 'llm_config': {'max_tokens': 2048, 'temperature': 0.7, 'OAI_CONFIG_LIST': '', 'DEFAULT_MODEL': 'gemini-pro'}, 'conversation_config': {'history_length': 10, 'cost_per_token': 2e-06}, 'logging_config': {'level': 'DEBUG', 'log_file': 'app.log'}, 'bedrock_config': {'ANTHROPIC_MODEL_ID': 'anthropic.claude-v2', 'GEMINI_MODEL_ID': 'google_gemini_pro'}, 'llm': {'provider': 'gemini', 'model': 'gemini-pro', 'api_key': '${GEMINI_API_KEY}', 'api_type': 'google'}, 'vector_stores': {'knowledge_base': {'type': 'memory', 'collection_name': 'knowledge_base', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'databricks_schema': {'type': 'memory', 'collection_name': 'databricks_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'graphql_schema': {'type': 'memory', 'collection_name': 'graphql_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}}, 'agents': {'knowledge_retriever': {'system_prompt': 'You are a knowledge retrieval expert. Use the provided context to answer questions accurately.', 'max_tokens': 4000, 'temperature': 0.1}, 'sql_generator': {'system_prompt': 'You are a SQL expert. Generate accurate SQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'graphql_generator': {'system_prompt': 'You are a GraphQL expert. Generate accurate GraphQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'code_generator': {'system_prompt': 'You are an expert software architect and developer. Generate high-quality, production-ready code.', 'max_tokens': 4000, 'temperature': 0.1}, 'orchestrator': {'system_prompt': 'You are an AI orchestrator. Analyze user questions and delegate tasks to appropriate agents.', 'max_tokens': 4000, 'temperature': 0.1}}}\n",
      "DEBUG:autogen_app.agents:Initializing KnowledgeRetrieverAgent with config: {'database_config': {'db_type': 'sqlite', 'db_url': 'sqlite:///local.db', 'pg_db_url': 'postgresql://user:password@localhost:5432/mydatabase', 'pg_collection_name': 'default_collection'}, 'llm_config': {'max_tokens': 2048, 'temperature': 0.7, 'OAI_CONFIG_LIST': '', 'DEFAULT_MODEL': 'gemini-pro'}, 'conversation_config': {'history_length': 10, 'cost_per_token': 2e-06}, 'logging_config': {'level': 'DEBUG', 'log_file': 'app.log'}, 'bedrock_config': {'ANTHROPIC_MODEL_ID': 'anthropic.claude-v2', 'GEMINI_MODEL_ID': 'google_gemini_pro'}, 'llm': {'provider': 'gemini', 'model': 'gemini-pro', 'api_key': '${GEMINI_API_KEY}', 'api_type': 'google'}, 'vector_stores': {'knowledge_base': {'type': 'memory', 'collection_name': 'knowledge_base', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'databricks_schema': {'type': 'memory', 'collection_name': 'databricks_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'graphql_schema': {'type': 'memory', 'collection_name': 'graphql_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}}, 'agents': {'knowledge_retriever': {'system_prompt': 'You are a knowledge retrieval expert. Use the provided context to answer questions accurately.', 'max_tokens': 4000, 'temperature': 0.1}, 'sql_generator': {'system_prompt': 'You are a SQL expert. Generate accurate SQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'graphql_generator': {'system_prompt': 'You are a GraphQL expert. Generate accurate GraphQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'code_generator': {'system_prompt': 'You are an expert software architect and developer. Generate high-quality, production-ready code.', 'max_tokens': 4000, 'temperature': 0.1}, 'orchestrator': {'system_prompt': 'You are an AI orchestrator. Analyze user questions and delegate tasks to appropriate agents.', 'max_tokens': 4000, 'temperature': 0.1}}}\n",
      "DEBUG:autogen_app.agents:Vector store initialized: <autogen_app.vector_store.MemoryVectorStore object at 0x143117500>\n",
      "DEBUG:autogen_app.agents:Initializing SQLGeneratorAgent with config: {'database_config': {'db_type': 'sqlite', 'db_url': 'sqlite:///local.db', 'pg_db_url': 'postgresql://user:password@localhost:5432/mydatabase', 'pg_collection_name': 'default_collection'}, 'llm_config': {'max_tokens': 2048, 'temperature': 0.7, 'OAI_CONFIG_LIST': '', 'DEFAULT_MODEL': 'gemini-pro'}, 'conversation_config': {'history_length': 10, 'cost_per_token': 2e-06}, 'logging_config': {'level': 'DEBUG', 'log_file': 'app.log'}, 'bedrock_config': {'ANTHROPIC_MODEL_ID': 'anthropic.claude-v2', 'GEMINI_MODEL_ID': 'google_gemini_pro'}, 'llm': {'provider': 'gemini', 'model': 'gemini-pro', 'api_key': '${GEMINI_API_KEY}', 'api_type': 'google'}, 'vector_stores': {'knowledge_base': {'type': 'memory', 'collection_name': 'knowledge_base', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'databricks_schema': {'type': 'memory', 'collection_name': 'databricks_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'graphql_schema': {'type': 'memory', 'collection_name': 'graphql_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}}, 'agents': {'knowledge_retriever': {'system_prompt': 'You are a knowledge retrieval expert. Use the provided context to answer questions accurately.', 'max_tokens': 4000, 'temperature': 0.1}, 'sql_generator': {'system_prompt': 'You are a SQL expert. Generate accurate SQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'graphql_generator': {'system_prompt': 'You are a GraphQL expert. Generate accurate GraphQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'code_generator': {'system_prompt': 'You are an expert software architect and developer. Generate high-quality, production-ready code.', 'max_tokens': 4000, 'temperature': 0.1}, 'orchestrator': {'system_prompt': 'You are an AI orchestrator. Analyze user questions and delegate tasks to appropriate agents.', 'max_tokens': 4000, 'temperature': 0.1}}}\n",
      "DEBUG:autogen_app.agents:Vector store initialized: <autogen_app.vector_store.MemoryVectorStore object at 0x168928e30>\n",
      "DEBUG:autogen_app.agents:Initializing GraphQLGeneratorAgent with config: {'database_config': {'db_type': 'sqlite', 'db_url': 'sqlite:///local.db', 'pg_db_url': 'postgresql://user:password@localhost:5432/mydatabase', 'pg_collection_name': 'default_collection'}, 'llm_config': {'max_tokens': 2048, 'temperature': 0.7, 'OAI_CONFIG_LIST': '', 'DEFAULT_MODEL': 'gemini-pro'}, 'conversation_config': {'history_length': 10, 'cost_per_token': 2e-06}, 'logging_config': {'level': 'DEBUG', 'log_file': 'app.log'}, 'bedrock_config': {'ANTHROPIC_MODEL_ID': 'anthropic.claude-v2', 'GEMINI_MODEL_ID': 'google_gemini_pro'}, 'llm': {'provider': 'gemini', 'model': 'gemini-pro', 'api_key': '${GEMINI_API_KEY}', 'api_type': 'google'}, 'vector_stores': {'knowledge_base': {'type': 'memory', 'collection_name': 'knowledge_base', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'databricks_schema': {'type': 'memory', 'collection_name': 'databricks_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}, 'graphql_schema': {'type': 'memory', 'collection_name': 'graphql_schema', 'embedding_model': 'all-MiniLM-L6-v2', 'dimensions': 384}}, 'agents': {'knowledge_retriever': {'system_prompt': 'You are a knowledge retrieval expert. Use the provided context to answer questions accurately.', 'max_tokens': 4000, 'temperature': 0.1}, 'sql_generator': {'system_prompt': 'You are a SQL expert. Generate accurate SQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'graphql_generator': {'system_prompt': 'You are a GraphQL expert. Generate accurate GraphQL queries based on the provided schema and requirements.', 'max_tokens': 4000, 'temperature': 0.1}, 'code_generator': {'system_prompt': 'You are an expert software architect and developer. Generate high-quality, production-ready code.', 'max_tokens': 4000, 'temperature': 0.1}, 'orchestrator': {'system_prompt': 'You are an AI orchestrator. Analyze user questions and delegate tasks to appropriate agents.', 'max_tokens': 4000, 'temperature': 0.1}}}\n",
      "DEBUG:autogen_app.agents:Vector store initialized: <autogen_app.vector_store.MemoryVectorStore object at 0x172482c00>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing direct similarity search:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieving knowledge for query: What are the main components of the project?\n",
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of results found: 2\n",
      "\n",
      "First result content:\n",
      "\n",
      "        Project Overview\n",
      "        Our project uses a microservices architecture with the following components:\n",
      "        - User Service: Handles user authentication and profile management\n",
      "        - Order Service: Manages order processing and fulfillment\n",
      "        - Payment Service: Handles payment processing and transactions\n",
      "        \n",
      "\n",
      "Testing agent's retrieve_knowledge method:\n",
      "Agent's knowledge retrieval result: [Document(id='43a35118-05dd-4267-9e1e-6eec8022c99d', metadata={'source': 'confluence', 'title': 'Project Architecture'}, page_content='\\n        Project Overview\\n        Our project uses a microservices architecture with the following components:\\n        - User Service: Handles user authentication and profile management\\n        - Order Service: Manages order processing and fulfillment\\n        - Payment Service: Handles payment processing and transactions\\n        '), Document(id='1362b473-bb53-42e0-bb2b-46636f106e98', metadata={'source': 'confluence', 'title': 'User Service API'}, page_content='\\n        API Documentation\\n        The User Service exposes the following endpoints:\\n        - POST /api/users: Create new user\\n        - GET /api/users/{id}: Get user details\\n        - PUT /api/users/{id}: Update user information\\n        ')]\n",
      "\n",
      "First result content:\n",
      "\n",
      "        Project Overview\n",
      "        Our project uses a microservices architecture with the following components:\n",
      "        - User Service: Handles user authentication and profile management\n",
      "        - Order Service: Manages order processing and fulfillment\n",
      "        - Payment Service: Handles payment processing and transactions\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Create all agents\n",
    "# Create all agents\n",
    "vector_stores = {\n",
    "    'knowledge_base': knowledge_store,\n",
    "    'databricks_schema': sql_store,\n",
    "    'graphql_schema': graphql_store\n",
    "}\n",
    "agents = create_agents(vector_stores)\n",
    "\n",
    "# Test knowledge retrieval\n",
    "print(\"Testing direct similarity search:\")\n",
    "results = knowledge_store.similarity_search(\"What are the main components of the project?\")\n",
    "print(f\"Number of results found: {len(results)}\")\n",
    "if results:\n",
    "    print(\"\\nFirst result content:\")\n",
    "    print(results[0].page_content)\n",
    "\n",
    "print(\"\\nTesting agent's retrieve_knowledge method:\")\n",
    "knowledge_retriever = agents['knowledge_retriever']\n",
    "agent_results = knowledge_retriever.retrieve_knowledge(\"What are the main components of the project?\")\n",
    "print(f\"Agent's knowledge retrieval result: {agent_results}\")\n",
    "if agent_results:\n",
    "    print(\"\\nFirst result content:\")\n",
    "    print(agent_results[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knowledge_retriever': <autogen_app.agents.KnowledgeRetrieverAgent at 0x110574b90>,\n",
       " 'sql_generator': <autogen_app.agents.SQLGeneratorAgent at 0x110577950>,\n",
       " 'graphql_generator': <autogen_app.agents.GraphQLGeneratorAgent at 0x173db8bc0>,\n",
       " 'code_generator': <autogen_app.agents.CodeGeneratorAgent at 0x173db9340>,\n",
       " 'orchestrator': <autogen_app.agents.OrchestratorAgent at 0x174070200>}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieving knowledge for query: What are the main components of the project?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Knowledge Retriever:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source: confluence\n",
      "Title: Project Architecture\n",
      "Content: \n",
      "        Project Overview\n",
      "        Our project uses a microservices architecture with the following components:\n",
      "        - User Service: Handles user authentication and profile management\n",
      "        - Order Service: Manages order processing and fulfillment\n",
      "        - Payment Service: Handles payment processing and transactions\n",
      "        \n",
      "\n",
      "Source: confluence\n",
      "Title: User Service API\n",
      "Content: \n",
      "        API Documentation\n",
      "        The User Service exposes the following endpoints:\n",
      "        - POST /api/users: Create new user\n",
      "        - GET /api/users/{id}: Get user details\n",
      "        - PUT /api/users/{id}: Update user information\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Test knowledge retriever\n",
    "print(\"Testing Knowledge Retriever:\")\n",
    "knowledge = agents['knowledge_retriever'].retrieve_knowledge(\"What are the main components of the project?\")\n",
    "for doc in knowledge:\n",
    "    print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "    print(f\"Title: {doc.metadata['title']}\")\n",
    "    print(f\"Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieving SQL schema for query: users table schema\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing SQL Generator:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source: databricks\n",
      "Table: users\n",
      "Content: \n",
      "        Users Table Schema\n",
      "        CREATE TABLE users (\n",
      "            user_id INT PRIMARY KEY,\n",
      "            username VARCHAR(50),\n",
      "            email VARCHAR(100),\n",
      "            created_at TIMESTAMP\n",
      "        )\n",
      "        \n",
      "\n",
      "Source: databricks\n",
      "Table: orders\n",
      "Content: \n",
      "        Orders Table Schema\n",
      "        CREATE TABLE orders (\n",
      "            order_id INT PRIMARY KEY,\n",
      "            user_id INT,\n",
      "            total_amount DECIMAL(10,2),\n",
      "            status VARCHAR(20),\n",
      "            created_at TIMESTAMP,\n",
      "            FOREIGN KEY (user_id) REFERENCES users(user_id)\n",
      "        )\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Test SQL generator\n",
    "print(\"\\nTesting SQL Generator:\")\n",
    "schema_context = agents['sql_generator'].get_schema_context(\"users table schema\")\n",
    "for doc in schema_context:\n",
    "    print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "    print(f\"Table: {doc.metadata['table']}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieving GraphQL schema for query: user type definition\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing GraphQL Generator:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Source: graphql\n",
      "Type: User\n",
      "Content: \n",
      "        User Type\n",
      "        type User {\n",
      "            id: ID!\n",
      "            username: String!\n",
      "            email: String!\n",
      "            orders: [Order!]!\n",
      "        }\n",
      "        \n",
      "\n",
      "Source: graphql\n",
      "Type: Order\n",
      "Content: \n",
      "        Order Type\n",
      "        type Order {\n",
      "            id: ID!\n",
      "            user: User!\n",
      "            totalAmount: Float!\n",
      "            status: String!\n",
      "            createdAt: DateTime!\n",
      "        }\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Test GraphQL generator\n",
    "print(\"\\nTesting GraphQL Generator:\")\n",
    "schema_context = agents['graphql_generator'].get_schema_context(\"user type definition\")\n",
    "for doc in schema_context:\n",
    "    print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "    print(f\"Type: {doc.metadata['type']}\")\n",
    "    print(f\"Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test complex queries with orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieving knowledge for query: user service API documentation\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Orchestrator with Complex Query:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n",
      "DEBUG:autogen_app.agents:Retrieving SQL schema for query: users table schema\n",
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n",
      "DEBUG:autogen_app.agents:Retrieving GraphQL schema for query: user type definition\n",
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Knowledge from Confluence:\n",
      "\n",
      "Source: confluence\n",
      "Title: User Service API\n",
      "Content: \n",
      "        API Documentation\n",
      "        The User Service exposes the following endpoints:\n",
      "        - POST /api/users: Create new user\n",
      "        - GET /api/users/{id}: Get user details\n",
      "        - PUT /api/users/{id}: Update user information\n",
      "        \n",
      "\n",
      "Source: confluence\n",
      "Title: Project Architecture\n",
      "Content: \n",
      "        Project Overview\n",
      "        Our project uses a microservices architecture with the following components:\n",
      "        - User Service: Handles user authentication and profile management\n",
      "        - Order Service: Manages order processing and fulfillment\n",
      "        - Payment Service: Handles payment processing and transactions\n",
      "        \n",
      "\n",
      "SQL Schema:\n",
      "\n",
      "Source: databricks\n",
      "Table: users\n",
      "Content: \n",
      "        Users Table Schema\n",
      "        CREATE TABLE users (\n",
      "            user_id INT PRIMARY KEY,\n",
      "            username VARCHAR(50),\n",
      "            email VARCHAR(100),\n",
      "            created_at TIMESTAMP\n",
      "        )\n",
      "        \n",
      "\n",
      "Source: databricks\n",
      "Table: orders\n",
      "Content: \n",
      "        Orders Table Schema\n",
      "        CREATE TABLE orders (\n",
      "            order_id INT PRIMARY KEY,\n",
      "            user_id INT,\n",
      "            total_amount DECIMAL(10,2),\n",
      "            status VARCHAR(20),\n",
      "            created_at TIMESTAMP,\n",
      "            FOREIGN KEY (user_id) REFERENCES users(user_id)\n",
      "        )\n",
      "        \n",
      "\n",
      "GraphQL Schema:\n",
      "\n",
      "Source: graphql\n",
      "Type: User\n",
      "Content: \n",
      "        User Type\n",
      "        type User {\n",
      "            id: ID!\n",
      "            username: String!\n",
      "            email: String!\n",
      "            orders: [Order!]!\n",
      "        }\n",
      "        \n",
      "\n",
      "Source: graphql\n",
      "Type: Order\n",
      "Content: \n",
      "        Order Type\n",
      "        type Order {\n",
      "            id: ID!\n",
      "            user: User!\n",
      "            totalAmount: Float!\n",
      "            status: String!\n",
      "            createdAt: DateTime!\n",
      "        }\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Test complex query that requires multiple agents\n",
    "print(\"Testing Orchestrator with Complex Query:\")\n",
    "orchestrator = agents['orchestrator']\n",
    "\n",
    "# Example complex query\n",
    "complex_query = \"\"\"\n",
    "I need to understand how user data is structured in our system.\n",
    "Please provide:\n",
    "1. The database schema for users\n",
    "2. The GraphQL type definition for users\n",
    "3. Any relevant API documentation\n",
    "\"\"\"\n",
    "\n",
    "# Get knowledge from all relevant sources\n",
    "knowledge = orchestrator.knowledge_retriever.retrieve_knowledge(\"user service API documentation\")\n",
    "sql_schema = orchestrator.sql_generator.get_schema_context(\"users table schema\")\n",
    "graphql_schema = orchestrator.graphql_generator.get_schema_context(\"user type definition\")\n",
    "\n",
    "print(\"\\nKnowledge from Confluence:\")\n",
    "for doc in knowledge:\n",
    "    print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "    print(f\"Title: {doc.metadata['title']}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "\n",
    "print(\"\\nSQL Schema:\")\n",
    "for doc in sql_schema:\n",
    "    print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "    print(f\"Table: {doc.metadata['table']}\")\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "\n",
    "print(\"\\nGraphQL Schema:\")\n",
    "for doc in graphql_schema:\n",
    "    print(f\"\\nSource: {doc.metadata['source']}\")\n",
    "    print(f\"Type: {doc.metadata['type']}\")\n",
    "    print(f\"Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test code generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieving knowledge for query: project architecture\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Code Generator:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n",
      "DEBUG:autogen_app.agents:Retrieving SQL schema for query: users and orders tables\n",
      "DEBUG:autogen_app.agents:Retrieved 2 documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Code:\n",
      "\n",
      "Based on the following information:\n",
      "\n",
      "Project Architecture:\n",
      "\n",
      "        Project Overview\n",
      "        Our project uses a microservices architecture with the following components:\n",
      "        - User Service: Handles user authentication and profile management\n",
      "        - Order Service: Manages order processing and fulfillment\n",
      "        - Payment Service: Handles payment processing and transactions\n",
      "        \n",
      "\n",
      "Database Schema:\n",
      "\n",
      "        Orders Table Schema\n",
      "        CREATE TABLE orders (\n",
      "            order_id INT PRIMARY KEY,\n",
      "            user_id INT,\n",
      "            total_amount DECIMAL(10,2),\n",
      "            status VARCHAR(20),\n",
      "            created_at TIMESTAMP,\n",
      "            FOREIGN KEY (user_id) REFERENCES users(user_id)\n",
      "        )\n",
      "        \n",
      "\n",
      "Please generate a Python class for the User Service that:\n",
      "1. Implements user CRUD operations\n",
      "2. Uses SQLAlchemy for database access\n",
      "3. Follows best practices for error handling and logging\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test code generation with context from other agents\n",
    "print(\"Testing Code Generator:\")\n",
    "code_generator = agents['code_generator']\n",
    "\n",
    "# Get context from other agents\n",
    "knowledge = orchestrator.knowledge_retriever.retrieve_knowledge(\"project architecture\")\n",
    "sql_schema = orchestrator.sql_generator.get_schema_context(\"users and orders tables\")\n",
    "\n",
    "# Combine context for code generation\n",
    "context = \"\"\"\n",
    "Based on the following information:\n",
    "\n",
    "Project Architecture:\n",
    "{}\n",
    "\n",
    "Database Schema:\n",
    "{}\n",
    "\n",
    "Please generate a Python class for the User Service that:\n",
    "1. Implements user CRUD operations\n",
    "2. Uses SQLAlchemy for database access\n",
    "3. Follows best practices for error handling and logging\n",
    "\"\"\".format(\n",
    "    knowledge[0].page_content if knowledge else \"\",\n",
    "    sql_schema[0].page_content if sql_schema else \"\"\n",
    ")\n",
    "\n",
    "# Generate code\n",
    "print(\"\\nGenerated Code:\")\n",
    "print(context)  # In a real implementation, this would be sent to the code generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
