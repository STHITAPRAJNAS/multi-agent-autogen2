# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.15.2
#   kernelspec:
#     display_name: Python 3 (ipykernel)
#     language: python
#     name: python3
# ---

# %% [markdown]
# # Testing Agents and Prompts (Outside of FastAPI)
#
# This notebook lets you test the core logic and prompts of the application, such as agent interactions, knowledge retrieval, SQL generation, and code generation, step-by-step outside of the FastAPI environment.
#

# %%
# Setup
import logging
import os

# Set ENV to local, to use the local config
os.environ["ENV"] = "local"
# Import necessary libraries
from src.config_loader import ConfigLoader
from src.main import create_agents
from src.agents.knowledge_retriever import knowledge_retriever_helper
from src.agents.sql_generator import sql_generator_helper
from src.agents.code_generator import code_generator_helper
from src.utils.conversation_manager import ConversationManager

# Load configurations
config_loader = ConfigLoader()
bedrock_config = config_loader.get_bedrock_config()
llm_config = config_loader.get_llm_config()
app_config = config_loader.get_app_config()

# Load LLM configuration
model_type = app_config.get("MODEL_TYPE")
if model_type not in ["openai", "bedrock", "anthropic","gemini"]:
    raise ValueError(f"Invalid model type: {model_type}")

if model_type == "openai":
    try:
        from autogen import config_list_from_json
        config_list = config_list_from_json(env_or_file=llm_config.get("OAI_CONFIG_LIST"), filter_dict={"model": [llm_config.get("DEFAULT_MODEL")]})
    except Exception as e:
        raise
elif model_type == "bedrock":
    try:
        from autogen import config_list_bedrock
        config_list = config_list_bedrock(region_name=bedrock_config.get("BEDROCK_REGION"))
    except Exception as e:
        raise
elif model_type in ["anthropic","gemini"]:
    try:
        from src.utils.utils import config_list_from_models
        models = [bedrock_config.get("ANTHROPIC_MODEL_ID") if model_type == "anthropic" else bedrock_config.get("GEMINI_MODEL_ID")]
        config_list = config_list_from_models(models)
    except Exception as e:
        raise
else:
    raise ValueError(f"Invalid model type: {model_type}")
    
# Configure logging (optional, but useful)
logging.basicConfig(level=logging.INFO)

# %%
# Agent Creation
agents = create_agents(config_list, bedrock_config)
user_proxy = agents["user_proxy"]
knowledge_retriever = agents["knowledge_retriever"]
sql_generator = agents["sql_generator"]
code_generator = agents["code_generator"]

# %%
# ConversationManager instance
conversation_manager = ConversationManager()
conversation_state = conversation_manager.get_or_create_conversation_state("test_conversation")

# %% [markdown]
# ## Test Steps

# %% [markdown]
# ### Knowledge Retrieval

# %%
# Knowledge Retrieval Test
user_query_knowledge = "What is the capital of France?"
user_proxy.initiate_chat(knowledge_retriever, message=f"Retrieve knowledge about: {user_query_knowledge}")
knowledge_result = knowledge_retriever_helper(knowledge_retriever, user_query_knowledge)
print(f"Knowledge Retrieval Result:\n{knowledge_result}")
conversation_state.append_message(user_query_knowledge, knowledge_result, "knowledge_retrieval")

# %% [markdown]
# ### SQL Generation

# %%
# SQL Generation Test
user_query_sql = "Generate a SQL query to find all users in the 'users' table."
user_proxy.initiate_chat(sql_generator, message=f"Generate a SQL query for: {user_query_sql} conversation history: {conversation_state.get_history()}")
sql_result = sql_generator_helper(sql_generator, user_query_sql)
print(f"SQL Generation Result:\n{sql_result}")
conversation_state.append_message(user_query_sql, sql_result, "sql_generation")

# %% [markdown]
# ### Code Generation

# %%
# Code Generation Test
user_query_code = "Generate Python code to calculate the factorial of a number."
user_proxy.initiate_chat(code_generator, message=f"Generate Python code for: {user_query_code} conversation history: {conversation_state.get_history()}")
code_result = code_generator_helper(code_generator, user_query_code,conversation_state)
print(f"Code Generation Result:\n{code_result}")
conversation_state.append_message(user_query_code, code_result, "code_generation")

# %% [markdown]
# ### Conversation Manager

# %%
# Test conversation_manager
conversation_state.calculate_usage_cost(user_query_knowledge,knowledge_result,sql_result,code_result)
print(f"Conversation history {conversation_state.get_history()}")
print(f"Total cost {conversation_state.get_total_cost()}")
print(f"Total tokens {conversation_state.get_total_tokens()}")
print(f"Last message cost {conversation_state.get_last_message_cost()}")